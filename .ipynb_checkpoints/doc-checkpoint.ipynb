{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e87e9f-904e-4ad2-aa05-37e6f23010b1",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1: Data Preparation and Management for Yelp Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This task focuses on preparing the Yelp dataset for a sentiment analysis project. The goal is to classify Yelp reviews into three categories—positive, negative, and neutral—based on their content, targeting specifically restaurants and hotels reviews. This document outlines the steps taken to acquire, clean, preprocess, and prepare the dataset for subsequent modeling tasks.\n",
    "\n",
    "## Dataset Description and Acquisition\n",
    "\n",
    "- **Source**: The dataset was obtained from Yelp's Dataset Challenge, which is publicly available for educational and research purposes. It includes a comprehensive compilation of business reviews, user interactions, and metadata associated with Yelp businesses.\n",
    "- **Scope**: From the larger Yelp dataset, we filtered out reviews explicitly linked to restaurants and hotels, as these categories are most relevant to our sentiment analysis objectives.\n",
    "- **Volume**: After filtering, our dataset consists of approximately [X number of reviews], spanning from [start year] to [end year].\n",
    "\n",
    "## Detailed Data Cleaning and Preprocessing Steps\n",
    "\n",
    "### Text Cleaning\n",
    "\n",
    "1. **HTML Tag Removal**: Utilizing regular expressions, we stripped out any HTML tags that appear in the review texts, ensuring only textual content is retained for analysis.\n",
    "2. **Special Characters and Punctuation**: We removed special characters and punctuation, again using regular expressions, to focus on the words within the reviews.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- Employing the NLTK library, we tokenized the cleaned review texts into individual words. This step is crucial for breaking down the texts into manageable units for further processing.\n",
    "\n",
    "### Stop Words Removal\n",
    "\n",
    "- Common words that typically don’t contribute to sentiment (e.g., \"and\", \"is\", \"in\") were removed using NLTK’s predefined list of stop words. This helps reduce the dataset's noise, focusing on more meaningful words.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "- Words were converted to their lemma or dictionary form to consolidate different forms of a word into a base form. We used NLTK’s WordNetLemmatizer for this purpose.\n",
    "\n",
    "### Numerical Representation\n",
    "\n",
    "- The Tokenizer class from TensorFlow’s Keras API was utilized to convert text tokens into numerical format. This involves mapping each unique word to a unique integer and transforming the texts into sequences of these integers, making the data suitable for input into deep learning models.\n",
    "\n",
    "## Sentiment Labeling\n",
    "\n",
    "Based on the star ratings accompanying each review, we classified sentiments as follows:\n",
    "\n",
    "- **Positive**: Reviews rated with 4 or 5 stars.\n",
    "- **Negative**: Reviews rated with 1 or 2 stars.\n",
    "- **Neutral**: Reviews with a 3-star rating.\n",
    "\n",
    "This categorical labeling facilitates a supervised learning approach, allowing models to learn from labeled examples.\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "- **Missing Data**: Encountering reviews with missing or incomplete text posed a challenge. We opted to remove such instances to maintain the quality and consistency of our analysis.\n",
    "- **Large Vocabulary**: The wide variety of words in the reviews introduced challenges in memory usage and computational efficiency during the tokenization and numerical conversion process. To address this, we limited our vocabulary size to the top [X] most frequent words for the numerical representation, ensuring a balance between computational efficiency and retaining meaningful textual information.\n",
    "- **Class Imbalance**: The dataset exhibited a skew in the distribution of sentiments, with an overrepresentation of positive reviews. To mitigate potential biases, we plan to explore techniques such as class weighting during the model training phase to ensure a fair representation of each sentiment class.\n",
    "\n",
    "## Tools and Libraries Used\n",
    "\n",
    "- **Pandas** and **NumPy** for data manipulation.\n",
    "- **NLTK** for natural language processing tasks, including tokenization, stop words removal, and lemmatization.\n",
    "- **TensorFlow** and specifically the Keras API for preprocessing text data and preparing it for deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd3c75-39af-445f-a5ce-57e1d6a22ed4",
   "metadata": {},
   "source": [
    "# Documentation for `trainmodel.py`\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `trainmodel.py` module is an essential component of our sentiment analysis project, dedicated to preparing and processing textual data for training deep learning models, specifically LSTM networks. This module encapsulates the functionality required for data loading, preprocessing, tokenization, and preparation to ensure the data is suitable for model training.\n",
    "\n",
    "### Functions and Their Descriptions\n",
    "\n",
    "#### 1. `load_data(file_path)`\n",
    "This function is responsible for loading data from a CSV file into a pandas DataFrame, which serves as the primary data structure for further manipulations and analysis.\n",
    "\n",
    "**Parameters:**\n",
    "- **`file_path`** (str): The path to the CSV file.\n",
    "\n",
    "**Returns:**\n",
    "- **`DataFrame`**: A pandas DataFrame containing the loaded data.\n",
    "\n",
    "#### 2. `create_tokenizer(texts, max_vocab=MAX_VOCAB)`\n",
    "This function initializes and fits a Keras Tokenizer. It is configured to only consider the top `max_vocab` words ordered by word frequency across the texts. This tokenizer later transforms text strings into integer sequences.\n",
    "\n",
    "**Parameters:**\n",
    "- **`texts`** (list of str): List of text strings to tokenize.\n",
    "- **`max_vocab`** (int, optional): The maximum size of the vocabulary. Defaults to `MAX_VOCAB`.\n",
    "\n",
    "**Returns:**\n",
    "- **`Tokenizer`**: A fitted Keras Tokenizer instance.\n",
    "\n",
    "#### 3. `tokenize_and_pad(texts, tokenizer, max_length=MAX_LENGTH)`\n",
    "After texts are converted to integer sequences, this function pads or truncates them to a uniform length, which is crucial for batch processing in neural networks.\n",
    "\n",
    "**Parameters:**\n",
    "- **`texts`** (list of str): The text strings to tokenize and pad.\n",
    "- **`tokenizer`** (Tokenizer): The tokenizer to use for converting text to sequences.\n",
    "- **`max_length`** (int, optional): The maximum length of the sequences after padding. Defaults to `MAX_LENGTH`.\n",
    "\n",
    "**Returns:**\n",
    "- **`ndarray`**: An array of shape (n_samples, max_length) containing the padded sequences.\n",
    "\n",
    "#### 4. `compute_class_weights(labels)`\n",
    "To handle class imbalance effectively, this function calculates the weights for each class based on their frequency in the data. These weights can be used during model training to give higher priority to minority classes.\n",
    "\n",
    "**Parameters:**\n",
    "- **`labels`** (array-like): An array-like structure of class labels.\n",
    "\n",
    "**Returns:**\n",
    "- **`dict`**: A dictionary mapping class indices to their respective weights.\n",
    "\n",
    "#### 5. `prepare_input_data(file_path)`\n",
    "This high-level function orchestrates the data preparation process by calling the aforementioned functions sequentially: it loads data, replaces missing text values, tokenizes texts, pads sequences, and computes class weights.\n",
    "\n",
    "**Parameters:**\n",
    "- **`file_path`** (str): The path to the dataset in CSV format.\n",
    "\n",
    "**Returns:**\n",
    "- **`tuple`**: A tuple containing:\n",
    "  - **`X`** (ndarray): The tokenized and padded feature data.\n",
    "  - **`y`** (array): The target labels.\n",
    "  - **`word_index`** (dict): A dictionary mapping words to their integer indices.\n",
    "  - **`class_weights`** (dict): Weights for each class based on their frequency.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The `trainmodel.py` module plays a critical role for the sentiment analysis by ensuring that the input data is adequately prepared for training our LSTM models. By automating the preprocessing and preparation steps, this module helps streamline the workflow and ensures consistency and reproducibility in our model training processes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73ab8b-b3fa-4fac-b2a1-d844b743d698",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## LSTM Model for Sentiment Analysis\n",
    "\n",
    "### Introduction\n",
    "In this project, we developed a sentiment analysis model using Long Short-Term Memory (LSTM) networks. This type of recurrent neural network (RNN) is particularly suited to text data due to its ability to process sequences and remember previous information, which is crucial for understanding the context in textual data.\n",
    "\n",
    "### Data Preparation\n",
    "Data preparation involved using the `trainmodel.py` script, which performed several key functions:\n",
    "- **Data Loading:** Text data was loaded from a CSV file into a DataFrame.\n",
    "- **Text Preprocessing:** The text was cleaned, tokenized, and padded to ensure uniform sequence lengths.\n",
    "- **Class Weights Computation:** To address class imbalance, weights were computed for each sentiment class (Negative, Neutral, Positive), enhancing model fairness and accuracy.\n",
    "\n",
    "### LSTM Model Architecture\n",
    "The LSTM model was constructed with the following layers:\n",
    "- **Embedding Layer:** Converts text to fixed-size dense vectors. We used an embedding dimension of 64, allowing the model to learn an effective representation of words.\n",
    "- **LSTM Layer:** With 128 units, this layer processes the embeddings by capturing dependencies in text sequences.\n",
    "- **Dropout Layers:** Set at a rate of 0.5, these layers help prevent overfitting by randomly setting input units to 0 during training.\n",
    "- **Output Layers:** A Dense layer with 50 units followed by a ReLU activation, and a final Dense layer with 3 units (one for each sentiment class) with a softmax activation.\n",
    "\n",
    "### Model Compilation\n",
    "The model was compiled using the Adam optimizer and sparse categorical crossentropy as the loss function, suitable for multi-class classification tasks.\n",
    "\n",
    "### Training Process\n",
    "Training involved using K-Fold cross-validation with 5 folds to ensure robust evaluation across different subsets of data. Each fold of the training involved:\n",
    "- Splitting the data into training and validation subsets.\n",
    "- Training the model for up to 10 epochs with early stopping based on validation loss to prevent overfitting.\n",
    "\n",
    "#### Training Outputs:\n",
    "The training process highlighted model performance across different folds:\n",
    "- **Accuracy:** Started at 89.38% and reached up to 94.49% on training data across epochs.\n",
    "- **Validation Accuracy:** Showed a decrease from 88.41% to 86.78%, indicating potential overfitting as training progressed.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "Using the Keras Tuner, we optimized key model parameters:\n",
    "- **Embedding Dimension**\n",
    "- **LSTM Units**\n",
    "- **Dropout Rate**\n",
    "\n",
    "The best model achieved through hyperparameter tuning was then retrained on a larger subset of the data, ensuring it was finely tuned to the characteristics of our dataset.\n",
    "\n",
    "### Evaluation and Results\n",
    "The model's effectiveness was evaluated using a held-out test set:\n",
    "- **Confusion Matrix:**\n",
    "  ```\n",
    "  [[ 6066   688   409]\n",
    "   [  526  1410  1450]\n",
    "   [  178   407 19729]]\n",
    "  ```\n",
    "- **Classification Report:**\n",
    "  ```\n",
    "              precision    recall  f1-score   support\n",
    "    Negative       0.90      0.85      0.87      7163\n",
    "     Neutral       0.56      0.42      0.48      3386\n",
    "    Positive       0.91      0.97      0.94     20314\n",
    "\n",
    "    accuracy                           0.88     30863\n",
    "   macro avg       0.79      0.74      0.76     30863\n",
    "  weighted avg       0.87      0.88      0.87     30863\n",
    "  ```\n",
    "\n",
    "### Discussion\n",
    "The model demonstrated strong performance, especially in identifying positive sentiments, which could be attributed to the higher representation of this class in the dataset. The lower recall for the neutral class suggests difficulties in distinguishing neutral sentiments, potentially due to overlapping features with other classes.\n",
    "\n",
    "### Conclusion\n",
    "This LSTM model provides a robust framework for sentiment analysis, capable of effectively processing and classifying textual data. Future work could explore more sophisticated text preprocessing techniques, alternative RNN architectures like GRUs, or even transformer-based models for potentially better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcf921-241a-44b5-b8b7-35b86de00158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
