{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e87e9f-904e-4ad2-aa05-37e6f23010b1",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1: Data Preparation and Management for Yelp Sentiment Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This task focuses on preparing the Yelp dataset for a sentiment analysis project. The goal is to classify Yelp reviews into three categories—positive, negative, and neutral—based on their content, targeting specifically restaurants and hotels reviews. This document outlines the steps taken to acquire, clean, preprocess, and prepare the dataset for subsequent modeling tasks.\n",
    "\n",
    "## Dataset Description and Acquisition\n",
    "\n",
    "- **Source**: The dataset was obtained from Yelp's Dataset Challenge, which is publicly available for educational and research purposes. It includes a comprehensive compilation of business reviews, user interactions, and metadata associated with Yelp businesses.\n",
    "- **Scope**: From the larger Yelp dataset, we filtered out reviews explicitly linked to restaurants and hotels, as these categories are most relevant to our sentiment analysis objectives.\n",
    "- **Volume**: After filtering, our dataset consists of approximately [X number of reviews], spanning from [start year] to [end year].\n",
    "\n",
    "## Detailed Data Cleaning and Preprocessing Steps\n",
    "\n",
    "### Text Cleaning\n",
    "\n",
    "1. **HTML Tag Removal**: Utilizing regular expressions, we stripped out any HTML tags that appear in the review texts, ensuring only textual content is retained for analysis.\n",
    "2. **Special Characters and Punctuation**: We removed special characters and punctuation, again using regular expressions, to focus on the words within the reviews.\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "- Employing the NLTK library, we tokenized the cleaned review texts into individual words. This step is crucial for breaking down the texts into manageable units for further processing.\n",
    "\n",
    "### Stop Words Removal\n",
    "\n",
    "- Common words that typically don’t contribute to sentiment (e.g., \"and\", \"is\", \"in\") were removed using NLTK’s predefined list of stop words. This helps reduce the dataset's noise, focusing on more meaningful words.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "- Words were converted to their lemma or dictionary form to consolidate different forms of a word into a base form. We used NLTK’s WordNetLemmatizer for this purpose.\n",
    "\n",
    "### Numerical Representation\n",
    "\n",
    "- The Tokenizer class from TensorFlow’s Keras API was utilized to convert text tokens into numerical format. This involves mapping each unique word to a unique integer and transforming the texts into sequences of these integers, making the data suitable for input into deep learning models.\n",
    "\n",
    "## Sentiment Labeling\n",
    "\n",
    "Based on the star ratings accompanying each review, we classified sentiments as follows:\n",
    "\n",
    "- **Positive**: Reviews rated with 4 or 5 stars.\n",
    "- **Negative**: Reviews rated with 1 or 2 stars.\n",
    "- **Neutral**: Reviews with a 3-star rating.\n",
    "\n",
    "This categorical labeling facilitates a supervised learning approach, allowing models to learn from labeled examples.\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "- **Missing Data**: Encountering reviews with missing or incomplete text posed a challenge. We opted to remove such instances to maintain the quality and consistency of our analysis.\n",
    "- **Large Vocabulary**: The wide variety of words in the reviews introduced challenges in memory usage and computational efficiency during the tokenization and numerical conversion process. To address this, we limited our vocabulary size to the top [X] most frequent words for the numerical representation, ensuring a balance between computational efficiency and retaining meaningful textual information.\n",
    "- **Class Imbalance**: The dataset exhibited a skew in the distribution of sentiments, with an overrepresentation of positive reviews. To mitigate potential biases, we plan to explore techniques such as class weighting during the model training phase to ensure a fair representation of each sentiment class.\n",
    "\n",
    "## Tools and Libraries Used\n",
    "\n",
    "- **Pandas** and **NumPy** for data manipulation.\n",
    "- **NLTK** for natural language processing tasks, including tokenization, stop words removal, and lemmatization.\n",
    "- **TensorFlow** and specifically the Keras API for preprocessing text data and preparing it for deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd3c75-39af-445f-a5ce-57e1d6a22ed4",
   "metadata": {},
   "source": [
    "# Documentation for `trainmodel.py`\n",
    "\n",
    "## Overview\n",
    "\n",
    "`trainmodel.py` serves as a foundational script in our sentiment analysis project, handling the acquisition, cleaning, preprocessing, and preparation of Yelp review data. Its primary function is to prepare the dataset for subsequent deep learning models, ensuring data is in the correct format for effective model training.\n",
    "\n",
    "## Functions\n",
    "\n",
    "### `load_data(file_path)`\n",
    "\n",
    "- **Purpose**: Loads the dataset from a specified CSV file path.\n",
    "- **Input**: `file_path` (str) - The path to the CSV file containing the Yelp reviews.\n",
    "- **Output**: `DataFrame` - A pandas DataFrame containing the loaded dataset.\n",
    "- **Description**: This function reads a CSV file into a pandas DataFrame, making the dataset available for further processing steps. It ensures that the dataset is accessible and in a manipulable format.\n",
    "\n",
    "### `prepare_input_data(train_data_path, test_data_path)`\n",
    "\n",
    "- **Purpose**: Prepares the training and testing datasets for model input.\n",
    "- **Input**:\n",
    "  - `train_data_path` (str) - The path to the training data CSV file.\n",
    "  - `test_data_path` (str) - The path to the testing data CSV file.\n",
    "- **Output**: Tuple containing:\n",
    "  - `train_texts` (List[str]) - Preprocessed training text data.\n",
    "  - `train_labels` (List[int]) - Corresponding labels for the training data.\n",
    "  - `test_texts` (List[str]) - Preprocessed testing text data.\n",
    "  - `test_labels` (List[int]) - Corresponding labels for the testing data.\n",
    "  - `word_index` (Dict) - A dictionary mapping words to their numerical index.\n",
    "- **Description**: This function encompasses the core preprocessing workflow, including text cleaning, tokenization, stop words removal, and numerical conversion. It splits the dataset into training and testing sets, ensuring each is properly preprocessed for model training.\n",
    "\n",
    "### `clean_text(texts)`\n",
    "\n",
    "- **Purpose**: Cleans the raw review texts by removing HTML tags, special characters, and converting all text to lowercase.\n",
    "- **Input**: `texts` (List[str]) - A list of review texts to be cleaned.\n",
    "- **Output**: `cleaned_texts` (List[str]) - The cleaned review texts.\n",
    "- **Description**: Applies regular expressions and other text processing techniques to clean the provided review texts, preparing them for further NLP tasks.\n",
    "\n",
    "### `tokenize_and_pad(texts)`\n",
    "\n",
    "- **Purpose**: Tokenizes the cleaned texts and pads them to a uniform length.\n",
    "- **Input**: `texts` (List[str]) - A list of cleaned review texts.\n",
    "- **Output**: Tuple containing:\n",
    "  - `padded_sequences` (ndarray) - Numerically encoded and padded text sequences.\n",
    "  - `word_index` (Dict) - A dictionary mapping words to their numerical index.\n",
    "- **Description**: Utilizes the Keras Tokenizer to convert text to sequences of integers, then pads these sequences to ensure uniform length across all texts.\n",
    "\n",
    "### `compute_class_weights(labels)`\n",
    "\n",
    "- **Purpose**: Computes class weights to address class imbalance in the training data.\n",
    "- **Input**: `labels` (List[int]) - The list of labels for the training data.\n",
    "- **Output**: `class_weights` (Dict) - A dictionary mapping class indices to their corresponding weight.\n",
    "- **Description**: Calculates weights for each class based on their frequency in the dataset, providing a mechanism to counteract the effects of class imbalance during model training.\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "During the development of `trainmodel.py`, several challenges were encountered:\n",
    "\n",
    "- **Data Cleaning Complexity**: The diversity of text in Yelp reviews required robust cleaning methods. Regular expressions and NLTK functions were employed to effectively clean and standardize the text data.\n",
    "- **Large Vocabulary**: The initial tokenization revealed a vast vocabulary, leading to high memory consumption. A decision was made to limit the tokenizer's vocabulary to the most frequent words, striking a balance between model complexity and performance.\n",
    "- **Class Imbalance**: An imbalance in sentiment labels was addressed by computing class weights, allowing the model to give more importance to underrepresented classes during training.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- **pandas**: For data loading and manipulation.\n",
    "- **NumPy**: For numerical operations.\n",
    "- **TensorFlow/Keras**: For text tokenization, sequence padding, and numerical data preparation.\n",
    "- **NLTK**: For natural language processing tasks such as stop words removal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ad845-de3e-4136-98cf-574b1a250aab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Model Training and Evaluation Documentation\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "This document outlines the training and evaluation process for a Long Short-Term Memory (LSTM) model developed for sentiment analysis on Yelp reviews, focusing on classifying sentiments as positive, negative, or neutral. The model's architecture consists of LSTM layers tailored to process sequential data inherent in text, aiming to capture the contextual nuances essential for accurate sentiment classification.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The model was trained over 10 epochs with a batch size of [specify batch size], employing the Adam optimizer and sparse categorical cross-entropy as the loss function. The training and validation datasets were prepared using preprocessed Yelp review data, aiming for a balanced representation of sentiment classes.\n",
    "\n",
    "### Training Results Summary\n",
    "\n",
    "- **Epochs 1-3**: The model showed rapid improvement, with accuracy increasing significantly and loss decreasing both on training and validation datasets. This phase marked the initial learning curve where the model started capturing the underlying sentiment patterns.\n",
    "- **Epoch 4-6**: While training accuracy continued to improve, validation accuracy peaked at epoch 3 and then showed a slight decline, with a corresponding increase in validation loss. This suggests the beginning of overfitting to the training data, where the model's generalization to unseen data started to decrease.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **Final Training Accuracy**: 85.13%\n",
    "- **Final Training Loss**: 0.4210\n",
    "- **Validation Accuracy**: 78.75% (at last epoch)\n",
    "- **Validation Loss**: 0.5135 (at last epoch)\n",
    "- **Test Accuracy**: 79.65%\n",
    "- **Test Loss**: 0.4744\n",
    "\n",
    "These metrics indicate that the model has learned to classify sentiments with a relatively high degree of accuracy, albeit showing signs of overfitting as evidenced by the validation and test performance.\n",
    "\n",
    "## Observations and Insights\n",
    "\n",
    "- The model's ability to improve significantly in the initial epochs is promising, demonstrating its capacity to learn from the Yelp review data effectively.\n",
    "- The onset of overfitting from epoch 4 onwards, as indicated by the divergence of training and validation metrics, suggests the model's increasing specialization to the training data, which could limit its applicability to new, unseen data.\n",
    "- The close alignment between validation and test metrics suggests that the validation set is a good representative of the test set, and the model's performance is consistent across unseen datasets.\n",
    "\n",
    "## Conclusions and Future Directions\n",
    "\n",
    "While the LSTM model has shown a commendable ability to classify sentiments within Yelp reviews, the training process revealed critical insights, particularly regarding the balance between learning and overfitting. For future iterations or models:\n",
    "\n",
    "- Implementing regularization techniques and early stopping could mitigate overfitting, enhancing the model's generalization capabilities.\n",
    "- Adjusting the learning rate dynamically or experimenting with the model's architecture might yield improvements in performance and efficiency.\n",
    "\n",
    "This documentation encapsulates the LSTM model's development and evaluation phases, providing a foundation for future enhancements and iterations. The insights gained from this process underscore the importance of continuous monitoring and adjustment in model training to achieve optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d9a39-ab74-42c3-bd69-133a89e7e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
